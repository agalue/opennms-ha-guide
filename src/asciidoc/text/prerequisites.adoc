
// Allow GitHub image rendering
:imagesdir: ../images

== Pre-requisites

_RHEL/CentOS_ Linux distribution will be used for this solution.  The following instructions are applicable to versions 6 and 7.

For references purposes, the following machines will used:

[options="header, autowidth"]
|===
| Name        | IP address
| _onmssrv01_ | 192.168.205.151
| _onmssrv02_ | 192.168.205.152
| _pgdbsrv01_ | 192.168.205.153
| _pgdbsrv02_ | 192.168.205.154
| _nfssrv01_  | 192.168.205.155
| _cassandrasrv01_  | 192.168.205.161
| _cassandrasrv02_  | 192.168.205.162
| _cassandrasrv03_  | 192.168.205.163
|===

The virtual IP will be: +

_onmssrvvip_ : `192.168.205.150`

Static IP addresses must be configured on each machine prior to starting the cluster.

NOTE: The cassandra servers can be used only with Horizon 17 / Meridian 2016 or newer.

=== Testing Lab Setup

Ignore this section to use physical servers for this deployment.

Download and install the following tools on a test system:

* Vagrant (http://www.vagrantup.com)
* VirtualBox (http://www.virtualbox.org)

Without _Cassandra_:

The test system must have at least 8 GB of RAM with a quad-core processor,
The test environment will require 5 VMs. Two VMs require 2GB for a total of 7GB of RAM.

With _Cassandra_:

The test system must have around 16GB of RAM with a quad-core processor,
The test environment will require 7 VMs. Two VMs require 2GB for a total of 9GB of RAM.

Create a directory and inside of it create a file called Vagrantfile, with the following content:

[source, ruby]
----
include::../files/prerequisites/Vagrantfile[]
----

NOTE: Remember to add a comment around the Cassandra servers, if you're not going to use them.

The above file will setup 5 machines using latest _CentOS 7_ (when not using _Cassandra_, or 7 otherwise).
In order to use _CentOS 6_, replace the value of the `config.vm.box` attribute from `centos/7` to `centos/6`.
To use _CentOS 6_ on some machines and _CentOS 7_ on other machines, add a `:box` attribute on the _JSON_ configuration for the systems using a different OS than the default (the default box is `config.vm.box`).

To startup the VMs, execute the following command:

[source, bash]
----
vagrant up
----

This will take several minutes to download the base image, create the 5 machines, and execute the provisioning on each of them.

To _SSH_ a specific box, use the `vagrant ssh` command follow by the name of the VM, for example:

[source, bash]
----
vagrant ssh onmssrv01
----

_Vagrant_ automatically generates a user called `vagrant` with _sudo_ access without password.

As almost all the commands used on this tutorial requires _root_ access, either become _root_ (and change the _root_ password on each VM), or use the _vagrant_ user with `sudo` for all the commands.

Make sure that _SELinux_ is configured to be _enforcing_ in `/etc/sysconfig/selinux` for full security, and restart the VMs if required.

To restart all the VMs with Vagrant:

[source, bash]
----
vagrant reload
----

In addition, the internal firewall must be enabled by default.
There is a section designated to explain how to properly configure the internal firewall on each _Linux_ distribution.

=== Hostnames

Before starting the setup process, it is important that all nodes in the setup can connect to each other via their hostname.
This can be achieve by either adding the involved hosts to the `/etc/hosts` file, or use the _FQDN_ registered on the _DNS_ servers.

If _DNS_ is not an option, the `/etc/hosts` should look like the following on each server:

[source]
----
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.205.150  onmssrvvip.local      onmssrvvip
192.168.205.151  onmssrv01.local       onmssrv01
192.168.205.152  onmssrv02.local       onmssrv02
192.168.205.153  pgdbsrv01.local       pgdbsrv01
192.168.205.154  pgdbsrv02.local       pgdbsrv02
192.168.205.155  nfssrv01.local        nfssrv01
192.168.205.161  cassandrasrv01.local  cassandrasrv01
192.168.205.162  cassandrasrv02.local  cassandrasrv02
192.168.205.163  cassandrasrv03.local  cassandrasrv03
----

Be sure that the domain is properly declared at `/etc/idmapd.conf` for all the hosts, including the _NFS_ server.
In this particular case, the domain is `local`:

[source]
----
[General]
#Verbosity = 0
# The following should be set to the local NFSv4 domain name
# The default is the host's DNS domain name.
Domain = local
----

To restart all the VMs with _Vagrant_:

[source, bash]
----
vagrant reload
----

=== Timezone and NTP

Make sure that all servers have the same timezone and the time is synchronized between all of them.

[source]
----
rm -f /etc/localtime
ln -s /usr/share/zoneinfo/EST /etc/localtime
yum -y install ntp ntpdate
ntpdate -u pool.ntp.org
----

For RHEL/CentoOS 6 :

[source]
----
chkconfig ntpd on
service ntpd start
----

For RHEL/CentoOS 7 :

[source]
----
systemctl enable ntpd
systemctl start ntpd
----

IMPORTANT: This is mandatory for Cassandra.

=== PostgreSQL Repository

The standard _RHEL/CentOS_ repositories do not contain the packages for _pgpool-II_ or _repmgr_.
Additionally, the supported _PostgreSQL_ version for _RHEL/CentOS 6_ is _8.4_, and for _RHEL/CentOS 7_ is _9.2_.

Because this solution required the usage of at least _PostgreSQL 9.4_, _repmgr 3.0_ and _pgpool-II 3.4_, the  _PGDG_ repository needs to be added.
For this tutorial, we're going to use _PostgreSQL 9.5_ and the provided versions for _repmgr_ and _pgpool-II_ on the _PGDG_ repository for that version of PostgreSQL.
Newer versions can be used without issues, but if 9.4 will be used, make sure to use at least 9.4.11. Older versions cannot be used with this procedure.

_PGDG_ is a project that maintains _RPM_ builds of _PostgreSQL_ and related components.
More information and repo-URLâ€™s for _PGDG_ can be found here:

http://yum.postgresql.org/repopackages.php

Add the _PGDG_ repo on all nodes (as `root`):

[source, bash]
----
yum install https://download.postgresql.org/pub/repos/yum/9.5/redhat/rhel-7-x86_64/pgdg-redhat95-9.5-2.noarch.rpm -y
----

For _RHEL/CentOS 6_, replace `rhel-7` with `rhel-6`.

IMPORTANT: Make sure to install the above package on the following servers: `onmssrv01`, `onmssrv02`, `pgdbsrv01` and `pgdbsrv02`.

=== OpenNMS Repository

==== Horizon

The first step is to install the _opennms-repo_ _RPM_ appropriate for distribution in use.
This contains the information _YUM_ needs to get _OpenNMS_ package information for installing.

Find the appropriate release _RPM_ from link:http://yum.opennms.org/[yum.opennms.org].

Then install that repo package (as `root`):

[source, bash]
----
yum install http://yum.opennms.org/repofiles/opennms-repo-stable-rhel7.noarch.rpm -y
----

For _RHEL/CentOS 6_, replace `rhel-7` with `rhel-6`.

This solution has been tested with _OpenNMS Horizon 14_, and newer.

IMPORTANT: Make sure to install the above package on `onmssrv01` and `onmssrv02`.

==== Meridian

When purchasing a _Meridian_ license, credentials will be given to access the private repository that contains the _RPM_ packages.
With the private repository credentials, create a _YUM_ repository file called `/etc/yum.repos.d/opennms-meridian.repo` with the following content on each _OpenNMS_ server:

[source]
----
[meridian]
name=Meridian for Red Hat Enterprise Linux and CentOS
baseurl=https://username:password@meridian.opennms.com/packages/2015/stable/rhel$releasever
enabled=1
gpgcheck=1
gpgkey=http://yum.opennms.org/OPENNMS-GPG-KEY
----

Replace `username` and `password` with the proper credentials.

Replace the major version with the appropriate one. For the above example, Meridian 2015 will be used. To use a newer version, like Meridian 2016, replace the year on the URL.

IMPORTANT: Make sure to configure the above repository on `onmssrv01` and `onmssrv02`.

=== RPM Packages Installation

==== Database

The _pgpool-II_ servers (i.e. the _OpenNMS_ servers) are not going to run _PostgreSQL_, but it is recommended to install the `postgresql` packages on these servers.
For this reason, install the following packages on each of the _pgpool-II/OpenNMS_ servers:

[source]
----
yum install postgresql95 postgresql95-server postgresql95-libs pgpool-II-95 rsync -y
----

On each the database servers, install the packages for _PostgreSQL Server_ and _repmgr_:

[source]
----
yum install postgresql95 postgresql95-server postgresql95-contrib repmgr95 rsync -y
----

At this point, the `postgres` user must exist on all the 4 machines.
Verify with the following command:

[source, bash]
----
# grep postgres /etc/passwd
postgres:x:26:26:PostgreSQL Server:/var/lib/pgsql:/bin/bash
----

The user will be created automatically as part of the installation of the `postgresql95-server` package.

WARNING: Do not make _pgpool-II_ to start with the operating system, because it will be managed by the cluster services.

==== Java

IMPORTANT: Requires for _OpenNMS_ and _Cassandra_

Starting with _OpenNMS Horizon 16_ and _OpenNMS Meridian 2016_, _OpenNMS_ requires _Oracle JDK 8_.
For _Meridian 2015_ and older versions of _Horizon_, _Oracle JDK 7_ is required.
The appropriate _JDK_ will be installed with _OpenNMS_, as they are provided through the _OpenNMS YUM_ repositories.
Keep in mind that the provided Oracle JDK might not be the latest version, so it is recommended to install it directly from Oracle's web site.

Make sure to use the appropriate version for the Oracle JDK you're planning to use from http://java.oracle.com.

For example, to download and Install Oracle JDK 8:

[source, bash]
----
curl -v -L -H "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u91-b14/jdk-8u91-linux-x64.rpm -o /tmp/jdk-8u91-linux-x64.rpm
yum install -y /tmp/jdk-8u91-linux-x64.rpm
----

_Apache Cassandra_ also requires Java. It is recommended to download and install the Oracle JDK 8 on all the _Cassandra_ nodes if you're planning to use it.

==== OpenNMS

On each _OpenNMS Server_ install the appropriate packages.

For Horizon:

[source]
----
yum install opennms-core opennms-webapp-jetty 'perl(LWP)' 'perl(XML::Twig)' -y
----

For Meridian:

[source]
----
yum install meridian-core meridian-webapp-jetty 'perl(LWP)' 'perl(XML::Twig)' -y
----

_Meridian_ provides _RRDtool_ and it is enabled by default.

To use _RRDtool_ with _Horizon_, the _OpenNMS_ repository provides the packages as well as the libraries required to use it.

For `Horizon 16` or older:

[source, bash]
----
yum install jrrd rrdtool -y
----

For `Horizon 17` or newer:

[source, bash]
----
yum install jrrd2 rrdtool -y
----

WARNING: Do not make _OpenNMS_ to start with the operating system.  Instead it will be managed by the cluster services.

==== Haveged

It is recommended to install the `haveged` package on all the servers.

This service reduces the startup time of the _OpenNMS_ application.

This package belongs to the EPEL Repository (https://fedoraproject.org/wiki/EPEL).

Install and enable the EPEL Repository on RHEL/CentOS 6:

[source, bash]
----
yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm -y
yum install haveged -y
chkconfig haveged on
service haveged start
----

Install and enable the EPEL Repository on RHEL/CentOS 7:

[source, bash]
----
yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm -y
yum install haveged -y
systemctl enable haveged
systemctl start haveged
----

==== RedHat Cluster

The _High Availability YUM group_ must be installed on each cluster member (i.e. the _OpenNMS_ machines):

[source, bash]
----
[root@onmssrv01 ~]# yum groupinstall "High Availability" -y
[root@onmssrv02 ~]# yum groupinstall "High Availability" -y
----

IMPORTANT: By default, the above group is going to install the default cluster services for the selected _Linux_ distribution.
           For _CentOS/RHEL 6_, _CMAN/Ricci_; for _CentOS/RHEL 7_ _Pacemaker_.

==== NFS

NOTE: This applies only if you're planning to use _NFS_ for the shared storage.

On all the servers, make sure the following packages are installed:

* `nfs-utils`
* `nfs-utils-lib`

Then, on the _NFS_ server, enable and start the _NFS_ service:

On _RHEL/CentOS 6_:

[source, bash]
----
[root@nfssrv01 ~]# chkconfig nfs on
[root@nfssrv01 ~]# service nfs start
----

On _RHEL/CentOS 7_:

[source, bash]
----
[root@nfssrv01 ~]# systemctl enable rpcbind nfs-server
[root@nfssrv01 ~]# systemctl start rpcbind nfs-server
----

==== DRBD

NOTE: This applies only if you're planning to use _DRBD_ for the shared storage.

Install the _DRBD_ packages on both OpenNMS servers.

On _RHEL/CentOS 6_:

[source, bash]
----
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
yum install -y http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm
yum install -y kmod-drbd84 drbd84-utils
----

On _RHEL/CentOS 7_:

[source, bash]
----
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
yum install -y http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
yum install -y kmod-drbd84 drbd84-utils
----

WARNING: This solution has not been tested on RHEL/CentOS 6. The version of `drbd84-utils` shipped with RHEL/CentOS 7.1 has a bug in the Pacemaker integration script`. Until a fix is packaged, download the affected script directly from the upstream, on both nodes:

[source, bash]
----
curl -o /usr/lib/ocf/resource.d/linbit/drbd 'http://git.linbit.com/gitweb.cgi?p=drbd-utils.git;a=blob_plain;f=scripts/drbd.ocf;h=cf6b966341377a993d1bf5f585a5b9fe72eaa5f2;hb=c11ba026bbbbc647b8112543df142f2185cb4b4b'
----

_DRBD_ will not be able to run under the default _SELinux_ security policies. If you are familiar with _SELinux_, you can modify the policies in a more fine-grained manner, but here we will simply exempt _DRBD_ processes from _SELinux_ control:

[source, bash]
----
yum install -y policycoreutils-python
semanage permissive -a drbd_t
----

WARNING: Do not make _drbd_ to start with the operating system, because it will be managed by the cluster services.

==== Cassandra

NOTE: This applies only if you're planning to use _Newts_ as the storage technology for the performance metrics.
      This assumes you're going to use DRBD for the shared configuration.

Make sure that Oracle JDK 8 is installed on each _Cassandra Node_.

Configure the _Datastax_ repository, by creating a file at `/etc/yum.repos.d/datastax.repo` with the following content on each _Cassandra Node_:

[source]
----
[datastax]
name = DataStax Repo for Apache Cassandra
baseurl = http://rpm.datastax.com/community
enabled = 1
gpgcheck = 0
----

Install the  packages on each _Cassandra Node_:

[source, bash]
----
yum install -y cassandra30 cassandra30-tools
----

==== OS Updates

It is always recommended to keep the OS up to date, so at this point it is a good time to perform a _YUM_ update on all the servers, and reboot them.

[source, bash]
----
yum update -y
----

After updating all packages, it is recommended to reboot all machines.
For the _Vagrant lab_, use the following command:

[source, bash]
----
vagrant reload
----

=== Customization of Initialization Scripts

_Pgpool_ and _OpenNMS_ expect that _PostgreSQL_ is up and running, and all the connections are available prior starting the services.

To avoid potential problems with the _OpenNMS_ cluster when a machine dies, the following steps are necessary.

On each _OpenNMS_ server, create a script called `dbcleanup.sh` at `/usr/local/bin/` with the following content:

[source, bash]
----
include::../files/prerequisites/dbcleanup.sh[]
----

WARNING: Update the variables according to your environment.
         Make sure the permissions on the file are 0700:

[source, bash]
----
chmod 0700 /usr/local/bin/dbcleanup.sh
----

On each _PostgreSQL_ server, create a script called `dbcleanup.sh` at `/usr/local/bin` with the following content:

[source, bash]
----
include::../files/prerequisites/cleanup.sh[]
----

Make sure up update the ownership and the permissions on the file:

[source, bash]
----
chmod 0700 /usr/local/bin/dbcleanup.sh
chown postgres:postgres /usr/local/bin/dbcleanup.sh
----

It is important to tune the initialization scripts properly.

==== SysV Initialization Scripts (RHEL/CentOS 6)

Edit the file `/etc/init.d/pgpool-II-95`, and inside the start function, call the `dbcleanup.sh` script before start _pgpool_.
Here is how the content should look like:

[source, bash]
----
include::../files/prerequisites/pgpool[]
----

IMPORTANT: Remember to perform the above tasks on _onmssrv01_ and _onmssrv02_.

WARNING: Be careful when upgrading the installed packages! The files modified above may be overridden and will require updating again.

==== Systemd Initialization Scripts (RHEL/CentOS 7)

The _systemd_ initialization script for _pgpool-II_ is not correct and it must be fixed prior start using _pgpool_.
Edit the `/lib/systemd/system/pgpool-II-95.service` file and make sure the content looks like the following:

[source, ini]
----
include::../files/prerequisites/pgpool-II-95.service[]
----

Make sure the permissions on the file are correct:

[source, bash]
----
chmod 0644 /lib/systemd/system/pgpool-II-95.service
----

Edit `/usr/lib/tmpfiles.d/pgpool-II-95.conf` and make sure the content look like the following:

----
include::../files/prerequisites/pgpool-II-95.conf[]
----

To avoid reboot the system, fix the ownership of the directory:

[source, bash]
----
chown postgres:postgres /var/run/pgpool-II-95
----

It is important to update the default options used by `pgpool` when it initializes.
These options are defined on `/etc/sysconfig/pgpool-II-95`.
Be sure its content look like the following:

[source]
----
OPTS=" -n --discard-status"
----

Similarly, by default the _systemd_ initialization script for _OpenNMS_ will force `START_TIMEOUT=0`.
In order to avoid potential problems when checking the _OpenNMS_ state, it is better to disable this option.

Edit the `/usr/lib/systemd/system/opennms.service` file, remove the `-Q` from the `ExecStart` method; add `TimeoutStartSec=0` and replace `postgresql.service` with `pgpool-II-95.service`.
The content should look like this:

[source, ini]
----
include::../files/prerequisites/opennms.service[]
----

After updating the files, execute the following command:

[source, bash]
----
systemctl daemon-reload
----

IMPORTANT: Remember to perform the above tasks on _onmssrv01_ and _onmssrv02_.

WARNING: Be careful when upgrading the installed packages! The files modified above may be overridden and will require updating again.

=== Public Key Authentication

All the OpenNMS and PostgreSQL servers must connect to each other over _SSH_ without a password prompt for the _postgres_ user.
_SSH_ will be used to _rsync_ the data from the primary to the standby, and to initiate a failover from _pgpool-II_ servers.
Password-less _SSH_ can be achieved with public key authentication.

On each of the 4 servers (both _OpenNMS_ servers and both _DB_ servers), generate a new _SSH_ key for the _postgres_ user without passphrase:

[source, bash]
----
[root@pgdbsrv01 ~]# su - postgres -c "ssh-keygen -t rsa"
Generating public/private rsa key pair.
Enter file in which to save the key (/var/lib/pgsql/.ssh/id_rsa):
Created directory '/var/lib/pgsql/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /var/lib/pgsql/.ssh/id_rsa.
Your public key has been saved in /var/lib/pgsql/.ssh/id_rsa.pub.
----

Once all the _SSH_ keys have been created, add the generated public keys from all the hosts to `/var/lib/pgsql/.ssh/authorized_keys` on each host.
This will allow all machines to connect to each other and accept each host's key.

To show the public _RSA-key_, use the following command on each server:

[source, bash]
----
[root@pgdbsrv01 ~]# cat /var/lib/pgsql/.ssh/id_rsa.pub
----

Then, create the `authorized_keys` file, and put all the public keys inside (one per line).
The content of the file should look like this:

[source, bash]
----
[root@pgdbsrv01 ~]# cat /var/lib/pgsql/.ssh/authorized_keys
ssh-rsa AAAAB3...== postgres@onmssrv01.local
ssh-rsa AAAAB3...== postgres@onmssrv02.local
ssh-rsa AAAAB3...== postgres@pgdbsrv01.local
ssh-rsa AAAAB3...== postgres@pgdbsrv02.local
----

The actual keys have been omitted for simplification purposes.

Unattended access allows for no password or other validation.
Because this is desired, add all hosts to the `known_hosts` file.
This prevents the question to add the hosts fingerprint on the first connection.
Be sure to execute the following command on all the servers:

[source, bash]
----
[root@pgdbsrv01 ~]# su - postgres -c "ssh-keyscan -H {onmssrv01,onmssrv02,pgdbsrv01,pgdbsrv02}| tee ~/.ssh/known_hosts"
----

The `authorized_keys` and `known_host` on _pgdbsrv01_ are fine for all other hosts.
Use `scp` to copy these files to the other nodes:

[source, bash]
----
[root@pgdbsrv01 ~]# scp /var/lib/pgsql/.ssh/{authorized_keys,known_hosts} pgdbsrv02:/var/lib/pgsql/.ssh/
[root@pgdbsrv01 ~]# scp /var/lib/pgsql/.ssh/{authorized_keys,known_hosts} onmssrv01:/var/lib/pgsql/.ssh/
[root@pgdbsrv01 ~]# scp /var/lib/pgsql/.ssh/{authorized_keys,known_hosts} onmssrv02:/var/lib/pgsql/.ssh/
----

Be sure the to fix the permissions of the files on each server:

[source, bash]
----
[root@pgdbsrv01 ~]# chmod 600 /var/lib/pgsql/.ssh/authorized_keys
[root@pgdbsrv01 ~]# chown postgres:postgres -R /var/lib/pgsql/.ssh/*
[root@pgdbsrv01 ~]# restorecon -R  /var/lib/pgsql/
----

Now is a good time to test the configured authentication by trying to _SSH_ between all nodes as the _postgres_ user.
When working properly, the target host's prompt should return without any other validation:

[source, bash]
----
[root@pgdbsrv01 ~]# su - postgres
-bash-4.1$ ssh onmssrv01
Last login: Mon Jul 13 09:22:42 2015 from 192.168.205.163
-bash-4.1$ hostname
onmssrv01.local
-bash-4.1$ exit
logout
Connection to onmssrv01 closed.
-bash-4.1$ ssh onmssrv02
Last login: Mon Jul 13 09:22:53 2015 from 192.168.205.163
-bash-4.1$ hostname
onmssrv02.local
-bash-4.1$ exit
logout
Connection to onmssrv02 closed.
-bash-4.1$ ssh pgdbsrv02
Last login: Mon Jul 13 09:23:27 2015 from 192.168.205.163
-bash-4.1$ hostname
pgdbsrv02.local
-bash-4.1$ exit
logout
Connection to pgdbsrv02 closed.
-bash-4.1$ exit
logout
----

IMPORTANT: Repeat the above a similar set of commands from the rest of the servers to be sure that the _postgres_ user can _SSH_ without passwords.

=== Internal Firewall

==== RHEL/CentOS 6

The first thing to do is enable and start `iptables` on each server:

[source, bash]
----
/etc/init.d/iptables start
/etc/init.d/iptables save
chkconfig iptables on
----

The file `/etc/sysconfig/iptables` should now exist.
Edit this file on both servers, add the following content before the `COMMIT` instruction, and save the file:

[source]
----
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
----

The above instructions are the default content for a standard _CentOS 6_ box, so it could be possible that the above lines already exist on that file.
Using the _Vagrant lab_, _iptables_ is not enabled or configured by default, so the above lines are needed.

Copy `/etc/sysconfig/iptables` to the rest of the servers.
Then restart the _iptables_ service on all the machines.
This will ensure that the basic firewall configuration is the same on all servers.

The following are the ports to be opened for OpenNMS:

.Opened ports for OpenNMS
[option="header, autowidth"]
|===
| Port  | Protocol | Description
| 5817  |  TCP     | OpenNMS event listener
| 8980  |  TCP     | OpenNMS WebUI
| 18980 |  TCP     | OpenNMS JMX Management (internal monitoring)
| 8181  |  TCP     | OpenNMS JMX Configuration
| 9999  |  TCP     | pgpool-II (PostgreSQL cluster)
| 162   |  UDP     | SNMP Traps (To receive traps in OpenNMS)
|===

_RHEL 6_ uses _CMAN/Ricci_ and optionally _luci_ for managing the cluster.
The following are the ports to open on each cluster member (i.e. the _OpenNMS_ machines):

.Opened ports for Cluster management
[option="header, autowidth"]
|===
| Port       | Protocol | Description
| 5404, 5405 | UDP      | corosync/cman (Cluster Manager)
| 11111      | TCP      | ricci (propagates updated cluster information)
| 21064      | TCP      | dlm (Distributed Lock Manager)
| 16851      | TCP      | modclusterd
| 8084       | TCP      | _Optional_: luci (WebUI to configure the cluster)
| 7789       | TCP      | _Optional_: DRBD traffic
|===

On each _OpenNMS_ servers, edit the `/etc/sysconfig/iptables` file and add the following rules before the first `REJECT` entry:

[source]
----
-A INPUT -p udp -m multiport --dports 5404,5405 -j ACCEPT
-A INPUT -p tcp --dport 11111 -j ACCEPT
-A INPUT -p tcp --dport 21064 -j ACCEPT
-A INPUT -p tcp --dport 16851 -j ACCEPT
-A INPUT -p tcp --dport 8084 -j ACCEPT
-A INPUT -p tcp --dport 7789 -j ACCEPT

-A INPUT -p tcp --dport 5817 -j ACCEPT
-A INPUT -p tcp --dport 8980 -j ACCEPT
-A INPUT -p tcp --dport 18980 -j ACCEPT
-A INPUT -p tcp --dport 8181 -j ACCEPT
-A INPUT -p tcp --dport 9999 -j ACCEPT
-A INPUT -p udp --dport 162 -j ACCEPT
----

On each _PostgreSQL_ servers, edit the `/etc/sysconfig/iptables` file, and add the following rules before the first `REJECT` entry:

[source]
----
-A INPUT -p tcp --dport 5432 -j ACCEPT
----

On the _NFS_ server, edit the `/etc/sysconfig/iptables` file, and add the following rules before the first `REJECT` entry:

[source]
----
-A INPUT -p tcp --dport 2049 -j ACCEPT
----

On each _Cassandra Node_, several ports have to be opened:

.Opened ports for Cassandra
[option="header, autowidth"]
|===
| Port  | Protocol | Description
| 7199  | TCP      | JMX (was 8080 pre Cassandra 0.8.xx)
| 7000  | TCP      | Internode communication (not used if TLS enabled)
| 7001  | TCP      | TLS Internode communication (used if TLS enabled)
| 9160  | TCP      | hrift client API
| 9042  | TCP      | CQL native transport port
|===

To do this, edit the `/etc/sysconfig/iptables` file on each _Cassandra Node_, and add the following rules before the first `REJECT` entry:

[source]
----
-A INPUT -p tcp --dport 7199 -j ACCEPT
-A INPUT -p tcp --dport 7000 -j ACCEPT
-A INPUT -p tcp --dport 7001 -j ACCEPT
-A INPUT -p tcp --dport 9160 -j ACCEPT
-A INPUT -p tcp --dport 9042 -j ACCEPT
----

Finally, restart _iptables_ on all the servers:

[source, bash]
----
service iptables restart
----

==== RHEL/CentOS 7

The first thing to do is enable and start `firewalld` on each server:

[source, bash]
----
# systemctl enable firewalld
ln -s '/usr/lib/systemd/system/firewalld.service' '/etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service'
ln -s '/usr/lib/systemd/system/firewalld.service' '/etc/systemd/system/basic.target.wants/firewalld.service'

# systemctl start firewalld

# systemctl status firewalld
firewalld.service - firewalld - dynamic firewall daemon
   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled)
   Active: active (running) since Thu 2015-07-16 18:55:59 UTC; 4s ago
 Main PID: 29635 (firewalld)
   CGroup: /system.slice/firewalld.service
           â””â”€29635 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid

Jul 16 18:55:59 testsrv.local systemd[1]: Started firewalld - dynamic firewall daemon.
----

All firewall changes must be executed against _firewalld_ using `firewall-cmd` and not through _iptables_.

On each of the _OpenNMS_ servers, create a new file called `/etc/firewalld/services/opennms.xml`, and put the following content inside of it:

[source, xml]
----
include::../files/prerequisites/opennms.xml[]
----

Then, create a new file called `/etc/firewalld/services/drbd.xml`, and put the following content inside of it, only if you're planning to use _DRBD_:

[source, xml]
----
include::../files/prerequisites/drbd.xml[]
----

Then, reload firewall configuration on each _OpenNMS Server_:

[source, bash]
----
# firewall-cmd --reload
success
----

You can use the following command to verify that the new service called _opennms_ is listed:

[source, bash]
----
# firewall-cmd --get-services
RH-Satellite-6 amanda-client bacula bacula-client dhcp dhcpv6 dhcpv6-client dns ftp high-availability http https imaps ipp ipp-client ipsec kerberos kpasswd ldap ldaps libvirt libvirt-tls mdns mountd ms-wbt mysql nfs ntp opennms openvpn pmcd pmproxy pmwebapi pmwebapis pop3s postgresql proxy-dhcp radius rpc-bind samba samba-client smtp ssh telnet tftp tftp-client transmission-client vnc-server wbem-https
----

You should also see _drbd_ if it has been configured as well.

On each _OpenNMS Server_, enable the rules for the _opennms_ service:

[source, bash]
----
# firewall-cmd --permanent --add-service=opennms
success
# firewall-cmd --add-service=opennms
success
----

If apply, then enable the rules _drbd_ service:

[source, bash]
----
# firewall-cmd --permanent --add-service=drbd
success
# firewall-cmd --add-service=drbd
success
----

_RHEL 7_ uses _Pacemaker_ for managing the cluster.
There is already a service group for high availability applications that includes _Pacemaker_ ports.
To enable it on _firewalld_, do the following on each _OpenNMS Server_:

[source, bash]
----
# firewall-cmd --permanent --add-service=high-availability
success
# firewall-cmd --add-service=high-availability
success
----

Then, reload the firewall configuration on each _OpenNMS Server_:

[source, bash]
----
# firewall-cmd --reload
success
----

On each _PostgreSQL Server_, execute the following commands:

[source, bash]
----
# firewall-cmd --permanent --add-service=postgresql
success
# firewall-cmd --add-service=postgresql
success
# firewall-cmd --reload
success
----

On the _NFS_ server, execute the following commands:

[source, bash]
----
# firewall-cmd --permanent --add-service=nfs
success
# firewall-cmd --add-service=nfs
success
# firewall-cmd --reload
success
----

On the _Cassandra_ servers, create a new file called `/etc/firewalld/services/cassandra.xml`, and put the following content inside of it, only if you're planning to use _Cassandra_:

[source, xml]
----
include::../files/prerequisites/cassandra.xml[]
----

Then, reload firewall configuration:

[source, bash]
----
# firewall-cmd --reload
success
----

Then, enable the rules for the _cassandra_ service:

[source, bash]
----
# firewall-cmd --permanent --add-service=cassandra
success
# firewall-cmd --add-service=cassandra
success
# firewall-cmd --reload
success
----

==== Syslogd in OpenNMS

To use _Syslogd_ in OpenNMS for creating events from received _syslog_ messages, an additional port needs to be opened.
By default the port is 10514, but it could be different.
This port is configured in `/opt/opennms/etc/syslogd-configuration.xml`.

If the port is changed, then the firewall rules for _OpenNMS_ must be updated at `/etc/firewalld/services/opennms.xml`.

If the standard _syslog_ server on the _OpenNMS_ servers will be used as a gateway to receive _syslog_ messages from all other servers and then forward them to _OpenNMS_, the port UDP 514 must be opened.
You can either create a firewall _XML_ file for _syslog_, or add the port to `opennms.xml`.

If `opennms.xml` is updated, or another firewall rule _XML_ file is created, the firewall configuration needs to be reloaded:

[source, bash]
----
# firewall-cmd --reload
success
----

=== External Firewalls

It is important to allow communication between _OpenNMS_ and the nodes that are going to be monitored through the appropriate protocols.
Typical applications to open are _ICMP_, _SNMP_ (UDP 161) and _SNMP-Traps_ (UDP 162).
For other kind of monitor requirements, ports such as _HTTP_ (TCP 80), will need to be accessible.

In order to access _OpenNMS_, the _WebUI_ port must be opened.
By default it is 8980, but it can be changed.
In case the port is changed, the internal firewall rules for _OpenNMS_ must be updated.

Also, in some use cases, it is important to inject events directly to _OpenNMS_.
This is done through the port TCP 5417.

=== Multicast and Networking

_Corosync_ which is the common cluster service used on any version of _RHEL/CentOS_ requires that multicast communication is open across all the cluster members.
This is already covered by the firewall rules, but in case of a real environment, make sure that the IP rules on switches (or Virtual Networks, in case of _VMWare vCloud_) allows multicast; otherwise, the cluster wonâ€™t work.

It is extremely important to minimize network infrastructure between cluster members to reduce potential failure points.
